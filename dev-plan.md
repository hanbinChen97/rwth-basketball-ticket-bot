Hanbinï¼Œè¿™ä¸ª**Iterative Development** (è¿­ä»£å¼€å‘) çš„æ€è·¯éå¸¸ä¸“ä¸šã€‚å¯¹äºçˆ¬è™«è¿™ç§é«˜åº¦ä¾èµ–ç›®æ ‡ç½‘ç«™ç»“æ„çš„ä»»åŠ¡ï¼Œ"Fetch -\> Analyze -\> Implement" çš„æµç¨‹æ˜¯æœ€ç¨³å¥çš„ã€‚

æ—¢ç„¶ä½ æŒ‡å®šäº† `uv` å’Œå…·ä½“çš„é¡¹ç›®ç»“æ„ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªä»»åŠ¡æ‹†è§£æˆ 3 ä¸ªé˜¶æ®µçš„ Promptã€‚ä½ å¯ä»¥æŒ‰é¡ºåºå‘ç»™ AIã€‚

-----

### ğŸ“‚ æ¨èçš„é¡¹ç›®ç»“æ„ (Project Structure)

åœ¨å¼€å§‹ä¹‹å‰ï¼Œä½ å¯ä»¥å…ˆè®© AI å¸®ä½ æ­å»ºè¿™ä¸ªè„šæ‰‹æ¶ã€‚

```text
my_ticket_bot/
â”œâ”€â”€ pyproject.toml       # uv ç®¡ç†ä¾èµ–
â”œâ”€â”€ uv.lock
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.toml    # å­˜æ”¾ URL, ç›®æ ‡æ—¶é—´, è¡¨å•å¡«å†™çš„ User Info
â”œâ”€â”€ data/                # å­˜æ”¾æŠ“å–ä¸‹æ¥çš„ HTML (ç”¨äºè°ƒè¯•/åˆ†æ)
â”‚   â””â”€â”€ debug_page.html
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ fetcher.py       # è´Ÿè´£ç½‘ç»œè¯·æ±‚ (GET/POST)
    â”œâ”€â”€ parser.py        # è´Ÿè´£è§£æ HTML (BS4)
    â””â”€â”€ main.py          # è°ƒåº¦é€»è¾‘
```

-----

### ğŸš€ Step 1: ç¯å¢ƒæ­å»º & åŸºç¡€æŠ“å– (Setup & Fetch)

**ç›®æ ‡**ï¼šåˆå§‹åŒ–é¡¹ç›®ï¼Œé…ç½® `uv`ï¼Œå¹¶æŠŠç›®æ ‡ç½‘é¡µåŸæœ¬åŸæ ·åœ°ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¾›ä¸‹ä¸€æ­¥åˆ†æã€‚

**Copy this Prompt to AI:**

> I am starting a Python project using `uv` for package management. I need to set up the environment and write a script to save a webpage for offline analysis.
>
> **Requirements:**
>
> 1.  **Environment:** Python 3.12+, managed by `uv`. Dependencies: `httpx`, `beautifulsoup4`, `loguru`, `toml` (or `tomli` if needed).
> 2.  **Structure:**
>       * `config/settings.toml`: Store `TARGET_URL` and `USER_HEADERS` (User-Agent, etc.).
>       * `src/fetcher.py`: A script that uses `httpx.Client` to visit `TARGET_URL` and save the raw HTML content to `data/debug_page.html`.
> 3.  **Action:**
>       * Provide the content for `pyproject.toml` (commands to add dependencies).
>       * Provide the code for `config/settings.toml` with dummy data.
>       * Provide the code for `src/fetcher.py`. It must handle basic errors (e.g., 404, 500) and log using `loguru`.

-----

### ğŸ” Step 2: é™æ€åˆ†æ & æå–ç­–ç•¥ (Analyze & Parse)

**ç›®æ ‡**ï¼šä½ è¿è¡Œäº† Step 1 çš„ä»£ç ï¼Œæ‹¿åˆ°äº† `data/debug_page.html`ã€‚ç°åœ¨æŠŠè¿™ä¸ª HTML æ–‡ä»¶çš„å…³é”®ç‰‡æ®µï¼ˆæˆ–è€…å‘Šè¯‰ AI ä½ è§‚å¯Ÿåˆ°çš„ç»“æ„ï¼‰å‘ç»™ AIï¼Œè®©å®ƒå†™è§£æå™¨ã€‚

**Copy this Prompt to AI:**

> Now I have the webpage HTML saved in `data/debug_page.html`. I need a parser to extract the necessary form data.
>
> **Task:**
> Write `src/parser.py`. It should read the local `data/debug_page.html` file (offline mode) and use `BeautifulSoup` to:
>
> 1.  Find the main booking `<form>`.
> 2.  Extract the `action` URL (where to POST).
> 3.  **Crucial:** Extract all `<input type="hidden">` fields (names and values), as these usually contain CSRF tokens or ViewStates.
> 4.  Identify the inputs where I need to fill in my user data (e.g., matching "Name", "Phone" in labels).
>
> **Output:**
> A function `parse_booking_form(html_content)` that returns a dictionary containing the `action_url` and a `payload` dict (merged hidden tokens + placeholders for user data). Print the result to the console so I can verify the extracted tokens.

*(æ³¨æ„ï¼šåœ¨æ‰§è¡Œè¿™ä¸€æ­¥æ—¶ï¼Œå¦‚æœ Step 1 æŠ“ä¸‹æ¥çš„ HTML è¿‡äºå¤æ‚ï¼Œä½ å¯ä»¥æŠŠ HTML ä¸­ `<form>` ç›¸å…³çš„éƒ¨åˆ†å¤åˆ¶ä¸€æ®µè´´åœ¨ Prompt åé¢ç»™ AI å‚è€ƒ)*

-----

### âš¡ Step 3: æ•´åˆ & å‘é€è¯·æ±‚ (Integration & Action)

**ç›®æ ‡**ï¼šè§£æå™¨å†™å¥½äº†ï¼Œç°åœ¨è¦ç»“åˆ Config é‡Œçš„çœŸå®æ•°æ®ï¼Œå®Œæˆâ€œå®šæ—¶ + å¡«è¡¨ + æäº¤â€çš„é—­ç¯ã€‚

**Copy this Prompt to AI:**

> The parser is working. Now I need the main execution logic in `src/main.py` to automate the booking process.
>
> **Logic Flow:**
>
> 1.  **Load Config:** Read `TARGET_TIME`, `USER_DATA` (my real details), and `TARGET_URL` from `config/settings.toml`.
> 2.  **Wait:** Implement a lightweight loop checking system time. When current time \>= `TARGET_TIME`, proceed immediately.
> 3.  **Fetch Live:** Use `src/fetcher.py` logic to get the *live* page (refreshing just as the time hits).
> 4.  **Parse:** Pass the live HTML to `src/parser.py` to get fresh tokens.
> 5.  **Merge:** Update the extracted payload with `USER_DATA` from config.
> 6.  **Submit:** Use `httpx` to POST the data to the `action_url`.
>
> **Requirements:**
>
>   * Use `httpx.Client` (or `AsyncClient` if you recommend it for speed) to maintain cookies between the GET (fetch form) and POST (submit) steps. This is critical.
>   * Log the server's response code and body to verify success.

-----

### ğŸ’¡ ç»™ä½ çš„å¼€å‘å»ºè®® (Dev Tips)

1.  **Config First**: æŠŠæ‰€æœ‰ä¼šå˜çš„ä¸œè¥¿ï¼ˆURL, æŠ¢ç¥¨æ—¶é—´, ä½ çš„èº«ä»½è¯/å§“åï¼‰éƒ½æ”¾åœ¨ `config/settings.toml` é‡Œã€‚è¿™æ ·ä½ è°ƒæ•´ç­–ç•¥æ—¶ä¸éœ€è¦æ”¹ä»£ç ã€‚
2.  **Dry Run (ç©ºè·‘)**: åœ¨ Step 3 å®Œæˆåï¼Œå…ˆä¸è¦ç­‰åˆ° 3 ç‚¹é’Ÿã€‚æ‰¾ä¸€ä¸ªæ— å…³ç´§è¦çš„æ—¶é—´ï¼ˆæ¯”å¦‚ 2:00ï¼‰ï¼ŒæŠŠ `TARGET_TIME` è®¾ä¸ºå½“å‰æ—¶é—´ï¼Œè·‘ä¸€æ¬¡çœ‹çœ‹èƒ½ä¸èƒ½æå–åˆ° Token å¹¶å‘é€è¯·æ±‚ï¼ˆè™½ç„¶ä¼šå¤±è´¥ï¼Œä½†èƒ½çœ‹åˆ°æœåŠ¡å™¨è¿”å›ä»€ä¹ˆï¼Œæ¯”å¦‚ "Not Open Yet" è¿˜æ˜¯ "403 Forbidden"ï¼‰ã€‚
3.  **Cookie Persistence**: å¦‚æœç½‘ç«™éœ€è¦ç™»å½•ï¼Œä½ å¯èƒ½éœ€è¦åœ¨ `config` é‡ŒåŠ ä¸€ä¸ª `COOKIE_STRING` å­—æ®µï¼Œç›´æ¥æŠŠæµè§ˆå™¨é‡Œç™»å½•å¥½çš„ Cookie å¤åˆ¶è¿›å»ï¼Œç„¶ååœ¨ `src/fetcher.py` åˆå§‹åŒ– headers æ—¶å¸¦ä¸Šã€‚è¿™æ˜¯æœ€ç®€å•çš„è¿‡éªŒè¯æ–¹æ³•ã€‚